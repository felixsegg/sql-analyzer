<!DOCTYPE html>

<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Evaluation Help</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
<b>Evaluation Help</b><br><br>
Evaluate generated SQL queries and export the scores as CSV.
<ul>
    <li>
        <b>Settings (⚙):</b><br>
        Open the dialog to choose the comparator, thread pool size, max repetitions (if applicable), which generated queries to include, and the CSV output folder.
    </li>
    <li>
        <b>Start:</b><br>
        Begins evaluation. A single dual progress bar shows job progress:<br>
        – <b>Gray</b> = comparisons started (submitted).<br>
        – <b>Red</b> = comparisons finished (completed and recorded).<br>
        Both reach 100% when done.
    </li>
    <li>
        <b>Rate limits:</b><br>
        If the selected method uses an LLM and a rate limit is hit, a countdown appears to the right (“retry in…”). While it runs,
        progress may still move a bit (in-flight calls can complete; some new calls may start if allowed). That’s expected.
    </li>
    <li>
        <b>Cancel:</b><br>
        Stops the ongoing job and clears the indicators.
    </li>
    <li>
        <b>Save:</b><br>
        After completion, exports results to the configured directory as a CSV file.
    </li>
</ul>
Comparators:
<ul>
    <li><b>LLM</b>: An LLM rates semantic similarity. Not fully deterministic; choose a low temperature for stable scoring.</li>
</ul>
Tips:
<ul>
    <li>Larger thread pools evaluate faster but can trigger API rate limits with the LLM comparator; smaller pools are slower but safer.</li>
    <li>“Max repetitions” controls how many retries are attempted if a score is unavailable; deterministic comparators don’t need retries.</li>
    <li>Be sure to set the CSV output path in Settings before saving.</li>
</ul>
</body>
</html>
